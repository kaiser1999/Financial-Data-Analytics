par(mfrow=c(1,3))
boxplot(Chosen_SPX$log_return, Chosen_HSI$log_return, Chosen_FTSE$log_return,
names=c("SPX", "HSI", "FTSE"), xlab="Index", ylab="Daily log-return",
frame=FALSE, col=c("#00AFBB", "#E7B800", "#FC4E07"), boxwex=0.75,
main=paste0("Boxplot.", year_name))
Chosen_SPX$Index <- "SPX"
Chosen_HSI$Index <- "HSI"
Chosen_FTSE$Index <- "FTSE"
AllIndex <- rbind(Chosen_SPX, Chosen_HSI, Chosen_FTSE)
AllIndex$Index <- factor(AllIndex$Index, c("SPX", "HSI", "FTSE"))
library("gplots")
plotmeans(log_return~Index, data=AllIndex, xlab="Index", ylab="Daily log-return",
main=paste0("Mean Plot with 95% CI.", year_name))
anova.test <- aov(log_return~Index, data=AllIndex)
summary(anova.test)
# Reject the null hypothesis?
summary(anova.test)[[1]][[1, "Pr(>F)"]] < 0.05
(tukey.test <- TukeyHSD(anova.test))
plot(tukey.test)
mu_SPX <- mean(Chosen_SPX$log_return)
mu_HSI <- mean(Chosen_HSI$log_return)
mu_FTSE <- mean(Chosen_FTSE$log_return)
SSE_SPX <- sum((Chosen_SPX$log_return - mu_SPX)^2)
SSE_HSI <- sum((Chosen_HSI$log_return - mu_HSI)^2)
SSE_FTSE <- sum((Chosen_FTSE$log_return - mu_FTSE)^2)
n_SPX <- length(Chosen_SPX$log_return)
n_HSI <- length(Chosen_HSI$log_return)
n_FTSE <- length(Chosen_FTSE$log_return)
(df <- length(AllIndex$log_return) - 3)
(Within_group_MSE <- (SSE_SPX + SSE_HSI + SSE_FTSE) / df)
# Harmonic mean used for the two sample sizes
SPX_HSI_SE_ANOVA <- sqrt(Within_group_MSE / (2/(1/n_SPX + 1/n_HSI)))
FTSE_SPX_SE_ANOVA <- sqrt(Within_group_MSE / (2/(1/n_FTSE + 1/n_SPX)))
FTSE_HSI_SE_ANOVA <- sqrt(Within_group_MSE / (2/(1/n_FTSE + 1/n_HSI)))
SPX_vs_HSI <- abs(mu_SPX - mu_HSI) / SPX_HSI_SE_ANOVA
FTSE_vs_SPX <- abs(mu_FTSE - mu_SPX) / FTSE_SPX_SE_ANOVA
FTSE_vs_HSI <- abs(mu_FTSE - mu_HSI) / FTSE_HSI_SE_ANOVA
1 - ptukey(q=SPX_vs_HSI, nmeans=3, df=df)
1 - ptukey(q=FTSE_vs_SPX, nmeans=3, df=df)
1 - ptukey(q=FTSE_vs_HSI, nmeans=3, df=df)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
################################################################################
d <- read.csv("stock_1999_2002.csv", row.names=1) # read in data file
d <- as.ts(d)
returns <- (lag(d) - d)/d
colnames(returns) <- paste0(colnames(d), "_Return")
u1 <- returns[,"HSBC_Return"]
u2 <- returns[,"CLP_Return"]
u3 <- returns[,"CK_Return"]
n_sim <- 1e4
par(mfrow=c(1,3))
# QQ plot for empirical marginals
col <- c("blue", "orange", "green")
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
for (k in 1:ncol(returns)){
q <- quantile(returns[,k], probs=i, type=4, names=FALSE)
qqplot(q, sort(returns[,k]), col=col[k],
xlab="Empirical quantiles", ylab="Returns quantiles",
main=paste0(colnames(returns)[k], "'s return Q-Q Plot"))
abline(lsfit(q, sort(returns[,k])), lwd=2)
}
par(mfrow=c(1, 1))
library(copula)  # Package for copula computation
empirical_marginals <- function(x) pobs(x)
empirical_quantile <- function(p, samples){
q <- matrix(NA, nrow=nrow(p), ncol=ncol(p))
for (k in 1:ncol(p)){
q[,k] <- quantile(samples[,k], probs=p[,k], type=4, names=FALSE)
}
return (q)
}
# using empirical as the marginal distribution
emp_u <- empirical_marginals(returns)
################################################################################
# Assume a normal-copula with ncol(d)=3
# P2p: array of elements of upper triangular matrix
N.cop <- normalCopula(dim=ncol(d), dispstr="un")
fit <- fitCopula(N.cop, emp_u, "ml")
(rho <- coef(fit))
N.cop_fit <- normalCopula(rho, dim=ncol(d), dispstr="un")
set.seed(4002)
# Generate random samples u~U(0, 1) from the fitted gaussian copula
u_sim_N <- rCopula(n_sim, N.cop_fit)
colnames(u_sim_N) <- colnames(d)
pairs(u_sim_N[1:1e3,], col="blue")        # only show the first 1000
cor(u_sim_N)
cor(returns)
# Obtain returns based on empirical marginals
return_sim_N <- empirical_quantile(u_sim_N, returns)
colnames(return_sim_N) <- colnames(d)
pairs(return_sim_N[1:1e3,], col="green")  # only show the first 1000
################################################################################
Mahalanobis2 <- function(X){
mu <- apply(X, 2, mean)
inv_Sig <- solve(cov(X))
X_minus_mu <- sweep(X, 2, mu, FUN="-")
return (rowSums((X_minus_mu %*% inv_Sig) * X_minus_mu))
}
QQ_Plot <- function(sim_data, raw_data, col="blue"){
n_days <- nrow(raw_data)
i <- ((1:n_days) - 0.5) / n_days
q <- quantile(sim_data, probs=i, type=4, names=FALSE)
qqplot(q, sort(raw_data), col=col,
xlab="Empirical quantiles", ylab="Returns quantiles",
main="Squared Mahalanobis Q-Q Plot with empeirical marginals")
abline(lsfit(q, sort(raw_data)), lwd=2)
}
returns_Mahalanobis2 <- Mahalanobis2(returns)
################################################################################
sim_N_Mahalanobis2 <- Mahalanobis2(return_sim_N)
QQ_Plot(sim_N_Mahalanobis2, returns_Mahalanobis2, col="blue")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
d <- read.csv("stock_1999_2002.csv", row.names=1) # read in data file
d <- as.ts(d)
returns <- (lag(d) - d)/d
colnames(returns) <- paste0(colnames(d), "_Return")
u1 <- returns[,"HSBC_Return"]
u2 <- returns[,"CLP_Return"]
u3 <- returns[,"CK_Return"]
n_sim <- 1e4
par(mfrow=c(1,3))
# QQ plot for empirical marginals
col <- c("blue", "orange", "green")
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
for (k in 1:ncol(returns)){
q <- quantile(returns[,k], probs=i, type=4, names=FALSE)
qqplot(q, sort(returns[,k]), col=col[k],
xlab="Empirical quantiles", ylab="Returns quantiles",
main=paste0(colnames(returns)[k], "'s return Q-Q Plot"))
abline(lsfit(q, sort(returns[,k])), lwd=2)
}
par(mfrow=c(1, 1))
library(copula)  # Package for copula computation
empirical_marginals <- function(x) pobs(x)
empirical_quantile <- function(p, samples){
q <- matrix(NA, nrow=nrow(p), ncol=ncol(p))
for (k in 1:ncol(p)){
q[,k] <- quantile(samples[,k], probs=p[,k], type=4, names=FALSE)
}
return (q)
}
# using empirical as the marginal distribution
emp_u <- empirical_marginals(returns)
################################################################################
# Assume a normal-copula with ncol(d)=3
# P2p: array of elements of upper triangular matrix
N.cop <- normalCopula(dim=ncol(d), dispstr="un")
fit <- fitCopula(N.cop, emp_u, "ml")
(rho <- coef(fit))
N.cop_fit <- normalCopula(rho, dim=ncol(d), dispstr="un")
set.seed(4002)
# Generate random samples u~U(0, 1) from the fitted gaussian copula
u_sim_N <- rCopula(n_sim, N.cop_fit)
colnames(u_sim_N) <- colnames(d)
pairs(u_sim_N[1:1e3,], col="blue")        # only show the first 1000
cor(u_sim_N)
cor(returns)
# Obtain returns based on empirical marginals
return_sim_N <- empirical_quantile(u_sim_N, returns)
colnames(return_sim_N) <- colnames(d)
pairs(return_sim_N[1:1e3,], col="green")  # only show the first 1000
################################################################################
Mahalanobis2 <- function(X){
mu <- apply(X, 2, mean)
inv_Sig <- solve(cov(X))
X_minus_mu <- sweep(X, 2, mu, FUN="-")
return (rowSums((X_minus_mu %*% inv_Sig) * X_minus_mu))
}
QQ_Plot <- function(sim_data, raw_data, col="blue"){
n_days <- length(raw_data)
i <- ((1:n_days) - 0.5) / n_days
q <- quantile(sim_data, probs=i, type=4, names=FALSE)
qqplot(q, sort(raw_data), col=col,
xlab="Empirical quantiles", ylab="Returns quantiles",
main="Squared Mahalanobis Q-Q Plot with empeirical marginals")
abline(lsfit(q, sort(raw_data)), lwd=2)
}
returns_Mahalanobis2 <- Mahalanobis2(returns)
################################################################################
sim_N_Mahalanobis2 <- Mahalanobis2(return_sim_N)
QQ_Plot(sim_N_Mahalanobis2, returns_Mahalanobis2, col="blue")
i <- ((1:n_days) - 0.5) / n_days
q <- qchisq(i, 3)
qqplot(q, sort(returns_Mahalanobis2), main="Chi2 Q-Q Plot")
abline(lsfit(q, sort(returns_Mahalanobis2)))
################################################################################
# Assume a t-copula  with ncol(d)=3
t.cop <- tCopula(dim=ncol(d), dispstr='un')
m <- pobs(returns)         # pseudo-observations
fit <- fitCopula(t.cop, m, "ml")
(rho <- coef(fit)[1:ncol(d)])
(df <- coef(fit)[length(coef(fit))])
t.cop_fit <- tCopula(dim=ncol(d), rho, df=df, dispstr="un")
# Generate random samples u~U(0, 1) from the fitted t copula
u_sim_t <- rCopula(n_sim, t.cop_fit)
colnames(u_sim_t) <- colnames(d)
pairs(u_sim_t[1:1e3,], col="blue")        # only show the first 1000
# Obtain returns based on empirical marginals
return_sim_t <- empirical_quantile(u_sim_t, returns)
colnames(return_sim_t) <- colnames(d)
pairs(return_sim_t[1:1e3,], col="green")  # only show the first 1000
################################################################################
sim_t_Mahalanobis <- Mahalanobis2(return_sim_t)
QQ_Plot(sim_t_Mahalanobis, returns_Mahalanobis2, col="orange")
################################################################################
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
q_N <- quantile(sim_N_Mahalanobis, probs=i, type=4, names=FALSE)
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
q_N <- quantile(sim_N_Mahalanobis2, probs=i, type=4, names=FALSE)
q_t <- quantile(sim_t_Mahalanobis2, probs=i, type=4, names=FALSE)
sim_t_Mahalanobis2 <- Mahalanobis2(return_sim_t)
QQ_Plot(sim_t_Mahalanobis2, returns_Mahalanobis2, col="orange")
################################################################################
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
q_N <- quantile(sim_N_Mahalanobis2, probs=i, type=4, names=FALSE)
q_t <- quantile(sim_t_Mahalanobis2, probs=i, type=4, names=FALSE)
linear_N <- lsfit(q_N, sort(returns_Mahalanobis2))
linear_t <- lsfit(q_t, sort(returns_Mahalanobis2))
par(mfrow=c(1, 1))
# plot theoretical quantiles starting from 10
idx_start <- min(which(q_N > 10), which(q_t > 10))
plot(sort(linear_N$residuals^2)[idx_start:n_days], ylim=c(0, 25),
ylab="squared residuals", names="Gaussian",
pch=1, cex=1.5)
points(sort(linear_t$residuals^2)[idx_start:n_days], names="t",
pch=4, cex=1.5, lwd=2)
legend("topleft", pch=c(1, 4), cex=1.5, lwd=c(1, 2),
legend=c("Gaussian", "t"), lty=0)
par(mfrow=c(1, 1))
# plot theoretical quantiles starting from 10
idx_start <- min(which(q_N > 10), which(q_t > 10))
plot(sort(linear_N$residuals^2)[idx_start:n_days], ylim=c(0, 25),
ylab="squared residuals",
pch=1, cex=1.5)
points(sort(linear_t$residuals^2)[idx_start:n_days],
pch=4, cex=1.5, lwd=2)
legend("topleft", pch=c(1, 4), cex=1.5, lwd=c(1, 2),
legend=c("Gaussian", "t"), lty=0)
sort(linear_N$residuals)
sort(linear_t$residuals)
sort(linear_t$residuals^2)
sort(linear_N$residuals^2)
coef(linear_N)
coef(linear_t)
linear_N$residuals
sort(linear_N$residuals^2)
sort(linear_t$residuals^2)
d <- read.csv("stock_1999_2002.csv", row.names=1) # read in data file
d <- as.ts(d)
returns <- (lag(d) - d)/d
colnames(returns) <- paste0(colnames(d), "_Return")
u1 <- returns[,"HSBC_Return"]
u2 <- returns[,"CLP_Return"]
u3 <- returns[,"CK_Return"]
n_sim <- 1e4
par(mfrow=c(1,3))
# QQ plot for empirical marginals
col <- c("blue", "orange", "green")
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
for (k in 1:ncol(returns)){
q <- quantile(returns[,k], probs=i, type=4, names=FALSE)
qqplot(q, sort(returns[,k]), col=col[k],
xlab="Empirical quantiles", ylab="Returns quantiles",
main=paste0(colnames(returns)[k], "'s return Q-Q Plot"))
abline(lsfit(q, sort(returns[,k])), lwd=2)
}
par(mfrow=c(1, 1))
library(copula)  # Package for copula computation
empirical_marginals <- function(x) pobs(x)
empirical_quantile <- function(p, samples){
q <- matrix(NA, nrow=nrow(p), ncol=ncol(p))
for (k in 1:ncol(p)){
q[,k] <- quantile(samples[,k], probs=p[,k], type=4, names=FALSE)
}
return (q)
}
# using empirical as the marginal distribution
emp_u <- empirical_marginals(returns)
################################################################################
# Assume a normal-copula with ncol(d)=3
# P2p: array of elements of upper triangular matrix
N.cop <- normalCopula(dim=ncol(d), dispstr="un")
fit <- fitCopula(N.cop, emp_u, "ml")
(rho <- coef(fit))
N.cop_fit <- normalCopula(rho, dim=ncol(d), dispstr="un")
set.seed(4002)
# Generate random samples u~U(0, 1) from the fitted gaussian copula
u_sim_N <- rCopula(n_sim, N.cop_fit)
colnames(u_sim_N) <- colnames(d)
pairs(u_sim_N[1:1e3,], col="blue")        # only show the first 1000
cor(u_sim_N)
cor(returns)
# Obtain returns based on empirical marginals
return_sim_N <- empirical_quantile(u_sim_N, returns)
colnames(return_sim_N) <- colnames(d)
pairs(return_sim_N[1:1e3,], col="green")  # only show the first 1000
################################################################################
Mahalanobis2 <- function(X){
mu <- apply(X, 2, mean)
inv_Sig <- solve(cov(X))
X_minus_mu <- sweep(X, 2, mu, FUN="-")
return (rowSums((X_minus_mu %*% inv_Sig) * X_minus_mu))
}
QQ_Plot <- function(sim_data, raw_data, col="blue"){
n_days <- length(raw_data)
i <- ((1:n_days) - 0.5) / n_days
q <- quantile(sim_data, probs=i, type=4, names=FALSE)
qqplot(q, sort(raw_data), col=col,
xlab="Empirical quantiles", ylab="Returns quantiles",
main="Squared Mahalanobis Q-Q Plot with empeirical marginals")
abline(lsfit(q, sort(raw_data)), lwd=2)
}
returns_MD2 <- Mahalanobis2(returns)
################################################################################
sim_N_MD2 <- Mahalanobis2(return_sim_N)
QQ_Plot(sim_N_MD2, returns_MD2, col="blue")
i <- ((1:n_days) - 0.5) / n_days
q <- qchisq(i, 3)
qqplot(q, sort(returns_MD2), main="Chi2 Q-Q Plot")
abline(lsfit(q, sort(returns_MD2)))
################################################################################
# Assume a t-copula  with ncol(d)=3
t.cop <- tCopula(dim=ncol(d), dispstr='un')
m <- pobs(returns)         # pseudo-observations
fit <- fitCopula(t.cop, m, "ml")
(rho <- coef(fit)[1:ncol(d)])
(df <- coef(fit)[length(coef(fit))])
t.cop_fit <- tCopula(dim=ncol(d), rho, df=df, dispstr="un")
# Generate random samples u~U(0, 1) from the fitted t copula
u_sim_t <- rCopula(n_sim, t.cop_fit)
colnames(u_sim_t) <- colnames(d)
pairs(u_sim_t[1:1e3,], col="blue")        # only show the first 1000
# Obtain returns based on empirical marginals
return_sim_t <- empirical_quantile(u_sim_t, returns)
colnames(return_sim_t) <- colnames(d)
pairs(return_sim_t[1:1e3,], col="green")  # only show the first 1000
################################################################################
sim_t_MD2 <- Mahalanobis2(return_sim_t)
QQ_Plot(sim_t_MD2, returns_MD2, col="orange")
################################################################################
n_days <- nrow(returns)
i <- ((1:n_days) - 0.5) / n_days
q_N <- quantile(sim_N_MD2, probs=i, type=4, names=FALSE)
q_t <- quantile(sim_t_MD2, probs=i, type=4, names=FALSE)
linear_N <- lsfit(q_N, sort(returns_MD2))
linear_t <- lsfit(q_t, sort(returns_MD2))
par(mfrow=c(1, 1))
# plot theoretical quantiles starting from 10
(idx_start <- min(which(q_N > 10), which(q_t > 10)))
plot(sort(linear_N$residuals^2)[idx_start:n_days], ylim=c(0, 25),
ylab="squared residuals",
pch=1, cex=1.5)
points(sort(linear_t$residuals^2)[idx_start:n_days],
pch=4, cex=1.5, lwd=2)
legend("topleft", pch=c(1, 4), cex=1.5, lwd=c(1, 2),
legend=c("Gaussian copula", "Student's t-copula"), lty=0)
quantile(returns_MD2, probs=i, type=4, names=FALSE)
pobs(returns)
?pobs
?ks.test
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
d <- read.csv("stock_1999_2002.csv", row.names=1)	  # read in data file
d <- as.ts(d)
u <- (lag(d) - d) / d
colnames(u) <- colnames(d)
library(zoo)
plot(zoo(d), plot.type="multiple", col=c("blue", "orange", "green"))
plot(zoo(u), plot.type="multiple", col=c("blue", "orange", "green"))
par(mfrow=c(3,2), mar=c(4,4,4,4))
# if the dist is normal, the plot should close to this line
# histogram; qq-normal plot; add a line for reference
hist(u[,"HSBC"]); qqnorm(u[,"HSBC"]); qqline(u[,"HSBC"])
hist(u[,"CLP"]); qqnorm(u[,"CLP"]); qqline(u[,"CLP"])
hist(u[,"CK"]); qqnorm(u[,"CK"]); qqline(u[,"CK"])
############################################################
shapiro.test(u[,"HSBC"])
shapiro.test(u[,"CLP"])
shapiro.test(u[,"CK"])
############################################################
u1 <- u[,"HSBC"]; u2 <- u[,"CLP"]; u3 <- u[,"CK"]
ks.test(u1, pnorm, mean=mean(u1), sd=sd(u1))
ks.test(u2, pnorm, mean=mean(u2), sd=sd(u2))
ks.test(u3, pnorm, mean=mean(u3), sd=sd(u3))
############################################################
library("tseries")
JB.test <- function(u){
z <- u - mean(u)            # Remove mean
n <- length(z)              # Sample size
s <- sd(z)*sqrt((n-1)/n)	# Population standard deviation
sk <- sum(z^3)/(n*s^3)      # Skewness
ku <- sum(z^4)/(n*s^4) - 3  # Excess Kurtosis
JB <- n * (sk^2/6 + ku^2/24)	# JB test statistics
p <- 1 - pchisq(JB, 2)      # chi-squared p-value
list("JB_stat"=JB, "p_value"=p)
}
JB.test(u[,"HSBC"])
jarque.bera.test(u[,"HSBC"])
JB.test(u[,"CLP"])
jarque.bera.test(u[,"CLP"])
JB.test(u[,"CK"])
jarque.bera.test(u[,"CK"])
############################################################
library("car")
QQt.plot <- function(u, comp=""){
z <- u - mean(u) # Remove mean
sz <- sort(z)		   # sort z
n <- length(z)		  # sample size
s <- sd(z)*sqrt((n-1)/n)	# Population standard deviation
ku <- sum(z^4)/(n*s^4) - 3	# Excess kurtosis
nu <- 6/ku + 4 # Degrees of freedom
i <- ((1:n)-0.5)/n  # create a vector of percentiles
q <- qt(i, nu)		  # percentile points from t(v)
plot(q, sz, main=paste("Self-defined t Q-Q Plot of ", comp, " Return"))
qqline(sz, distribution=function(p) qt(p, df=nu), probs=c(0.25, 0.75))
qqPlot(z, distribution="t", df=nu, envelope=FALSE, line="quartiles",
col.lines="black", lwd=1, cex=1, grid=FALSE, id=FALSE,
main=paste("t Q-Q Plot of ", comp, " Return"))
nu
}
par(mfrow=c(3,2), mar=c(4,4,4,4))
df_HSBC <- QQt.plot(u[,"HSBC"], comp="HSBC")
df_CLP <- QQt.plot(u[,"CLP"], comp="CLP")
df_CK <- QQt.plot(u[,"CK"], comp="CK")
############################################################
ks.test(u[,"HSBC"], pt, df_HSBC)
ks.test(u[,"CLP"], pt, df_CLP)
ks.test(u[,"CK"], pt, df_CK)
############################################################
n <- 180
u_180 <- tail(u, n)
mu_180 <- apply(u_180, 2, mean)
S_180 <- cov(u_180)
z_180 <- sweep(u_180, 2, mu_180)
d2_180 <- diag(z_180 %*% solve(S_180) %*% t(z_180))
sd2_180 <- sort(d2_180)		# sort d2 in ascendingly
i <- ((1:n)-0.5)/n		# create percentile vector
q <- qchisq(i,3)		# compute quantiles
par(mfrow=c(1,1))
qqplot(q, sd2_180, main="Chi2 Q-Q Plot")		# QQ-chisquare plot
qqline(sd2_180, distribution=function(p) qchisq(p, df=3))
############################################################
cor(u_180)
############################################################
pairs(u_180)
############################################################
par(mfrow=c(4,3), mar=c(4,4,4,4))
hist(d[,"HSBC"]); hist(d[,"CLP"]); hist(d[,"CK"])
qqnorm(d[,"HSBC"]); qqline(d[,"HSBC"])
qqnorm(d[,"CLP"]); qqline(d[,"CLP"])
qqnorm(d[,"CK"]); qqline(d[,"CK"])
plot(d[,"HSBC"], lag(d[,"HSBC"]))
plot(d[,"CLP"], lag(d[,"CLP"]))
plot(d[,"CK"], lag(d[,"CK"]))
plot(u[,"HSBC"], lag(u[,"HSBC"]))
plot(u[,"CLP"], lag(u[,"CLP"]))
plot(u[,"CK"], lag(u[,"CK"]))
############################################################
par(mfrow=c(3,3), mar=c(4,4,4,4))
acf(d[,"HSBC"]); acf(d[,"CLP"]); acf(d[,"CK"])
acf(u[,"HSBC"]); acf(u[,"CLP"]); acf(u[,"CK"])
acf(u[,"HSBC"]^2); acf(u[,"CLP"]^2); acf(u[,"CK"]^2)
############################################################
set.seed(4002)
mu_180 <- apply(u_180, 2, mean)
S_180 <- cov(u_180)
C_180 <- chol(S_180) # Cholesky decomposition of Sigma
# set s0 to the most recent price
s0 <- tail(d, 1)
s_pred <- c()
for (i in 1:90) {
z <- rnorm(3)
v <- mu_180 + t(C_180) %*% z
s1 <- s0 * (1 + t(v))	# new stock price
s_pred <- rbind(s_pred, s1)
s0 <- s1	# update s0
}
s_pred <- ts(s_pred, start=nrow(d)+1)
data <- ts.union(d, s_pred)
par(mfrow=c(1,1))
col <- c("blue", "orange", "green", "pink", "brown", "red")
plot(data, plot.type="s", col=col)
legend("topright", col=col, lty=1,
legend=c("HSBC", "CLP", "CK", "HSBC_pred", "CLP_pred", "CK_pred"))
############################################################
shapiro.test(u[,"HSBC"])
Shapiro_Test <- function(x, n_sim=100000){
z <- matrix(rnorm(length(x)*n_sim), ncol=length(x))
sz <- t(apply(z, 1, sort))
m <- apply(sz, 2, mean)
V <- cov(sz)
inv_V <- solve(V)
c <- (t(m) %*% inv_V %*% inv_V %*% m)^(1/2)
a <- t(m) %*% inv_V / as.numeric(c)
W <- sum(a * sort(x))^2/sum((x-mean(x))^2)
return (W)
}
(Shapiro_Test(u[,"HSBC"]))
ks.test(u[,"HSBC"], pt, df_HSBC)
ks.test(u[,"CLP"], pt, df_CLP)
ks.test(u[,"CK"], pt, df_CK)
t_HSBC <- u[,"HSBC"]/sd(u[,"HSBC"])*sqrt(df_HSBC/(df_HSBC-2))
ks.test(t_HSBC, pt, df_HSBC)
t_CLP <- u[,"CLP"]/sd(u[,"CLP"])*sqrt(df_CLP/(df_CLP-2))
ks.test(t_CLP, pt, df_CLP)
t_CK <- u[,"CK"]/sd(u[,"CK"])*sqrt(df_CK/(df_CK-2))
ks.test(t_CK, pt, df_CK)
