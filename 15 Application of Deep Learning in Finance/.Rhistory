pred <- round(fin.nn$fit)
table(df$HSI, pred)
library(nnet)
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
set.seed(4012)
df <- iris
classes <- factor(df$Species)
levels(classes) <- 1:length(levels(classes))
df$Species <- classes
df <- apply(df, 2, as.numeric)
# One hidden layer with 2 neurons and linear output unit; ncol(df)=5
iris.nn <- nnet(x=df[,-ncol(df)], y=df[,ncol(df)], size=2, linout=TRUE)
summary(iris.nn)
plot.nnet(iris.nn, wts.only = F)
df[c(1, 51, 101),]
sum((df[,ncol(df)] - iris.nn$fitted.values)^2)    # MSE
pred <- round(iris.nn$fit)	# round the fitted values
table(df[,ncol(df)], pred)		# classification table
################################################################################
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
iris.nn
set.seed(4012)
rnorm(13)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4012)
rnorm(17)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4012)
iris.nn <- nnet(x=df[,-ncol(df)], y=df[,ncol(df)], size=2, linout=TRUE)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
set.seed(4012)
df <- iris
classes <- factor(df$Species)
levels(classes) <- 1:length(levels(classes))
df$Species <- classes
df <- apply(df, 2, as.numeric)
# One hidden layer with 2 neurons and linear output unit; ncol(df)=5
iris.nn <- nnet(x=df[,-ncol(df)], y=df[,ncol(df)], size=2, linout=TRUE)
summary(iris.nn)
plot.nnet(iris.nn, wts.only = F)
df[c(1, 51, 101),]
sum((df[,ncol(df)] - iris.nn$fitted.values)^2)    # MSE
pred <- round(iris.nn$fit)	# round the fitted values
table(df[,ncol(df)], pred)		# classification table
################################################################################
set.seed(4012)
iris.nn <- nnet(x=df[,-ncol(df)], y=df[,ncol(df)], size=2, linout=TRUE)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4012)
rnorm(1)
rnorm(1)
set.seed(4012)
rnorm(2)
set.seed(4012)
rnorm(3)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4012)
rnorm(2)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4012)
rnorm(1)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
df <- read.csv("../Datasets/fin-ratio.csv")
set.seed(4012)
for (i in 1:100){
rnorm(i)
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
if (sum(pred) == 0) print(i)
}
?nnet
set.seed(4012)
for (i in 1:100){
rnorm(i)
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE, trace=F)
pred <- round(fin.nn$fit)
if (sum(pred) == 0) print(i)
}
set.seed(4012)
rnorm(83)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
for (i in 1:100){
set.seed(4012)
rnorm(i)
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE, trace=F)
pred <- round(fin.nn$fit)
if (sum(pred) == 0) print(i)
}
set.seed(4012)
rnorm(19)
df <- read.csv("../Datasets/fin-ratio.csv")
fin.nn <- nnet(x=df[,-ncol(df)], y=df$HSI, size=2, linout=TRUE)
summary(fin.nn)
pred <- round(fin.nn$fit)
table(df$HSI, pred)
set.seed(4002)
# logV starts from 0; y starts from 1
y <- array(0, 1000)
ln_v <- array(0, 1001)
for (i in 2:1001){
ln_v[i] <- 0.5 - 0.332 * ln_v[i-1] + 0.12 * rnorm(1, 0, 1)
y[i-1] <- sqrt(exp(ln_v[i])) * rnorm(1, 0, 1)
}
library(invgamma)
seed=4002; num_it=1e5
#MCMC_SV <- function(y, num_it=1e5, seed=4002){
MH_V <- function(logV_, y_t, phi, psi, lamb){
logV_old <- logV_[1]; logV_t <- logV_[2]; logV_new <- logV_[3]
mu_t <- (phi*(1-psi) + psi*(logV_new + logV_old)) / (1 + psi^2)
tilde_mu_t <- mu_t + lamb/2 * (y_t^2 * exp(-mu_t) - 1)
tilde_lambda <- lamb / (1 + psi^2)
logV_prop <- rnorm(1, tilde_mu_t, tilde_lambda)
nominator <- exp(-y_t^2/2 * exp(-logV_prop))
denominator <- exp(-y_t^2/2 * exp(-mu_t) * (1 + mu_t - logV_prop))
target_diff <- nominator / denominator
if (!is.na(target_diff)){
if (runif(1) < min(1, target_diff)){
return (logV_prop)
}
}
return (logV_t)
return (MH_V(logV_, y_t, phi, psi, lamb))
}
# prior: phi ~ N(0, 1); psi ~ N(0, 1); sigma ~ IG(2.5, 0.025)
mu_phi <- 0; sig2_phi <- 1; mu_psi <- 0; sig2_psi <- 1; alpha_lamb <- 2.5; beta_lamb <- 0.025
T <- length(y)
set.seed(seed)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
#phi <- 0.6 ; psi <- -0.4; lamb <- 0.2
logV_hist <- matrix(0, nrow=num_it, ncol=T+1)
phi_hist <- array(phi, dim=num_it)
psi_hist <- array(psi, dim=num_it)
lamb_hist <- array(lamb, dim=num_it)
for (t in 2:T+1){
logV_hist[1,t] <- phi + psi * logV_hist[1,t-1] + sqrt(lamb) * rnorm(1, 0, 1)
}
for (i in 2:num_it){
# MH sampling for ln V
logV <- c(logV_hist[i-1,], 0)
for (t in 1:T){
# logV starts from 0; y starts from 1
# a little bug for t = 0 and t = T + 1, but run it first
logV[t+1] <- MH_V(logV[c(t, t+1, t+2)], y[t], phi, psi, lamb)
}
logV <- logV[-(T+2)]
# Gibbs sampling for parameters
mu_phi <- (sig2_phi*sum(logV[-1]-psi*logV[-(T+1)]) + lamb*mu_phi)/(T*sig2_phi + lamb)
sig2_phi <- lamb*sig2_phi/(T*sig2_phi + lamb)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
mu_psi <- (sig2_psi*sum(logV[-(T+1)]*(logV[-1]-phi)) + lamb*mu_psi) / (sig2_psi*sum(logV[-(T+1)]^2) + lamb)
sig2_psi <- lamb*sig2_psi/(sig2_psi*sum(logV[-(T+1)]^2) + lamb)
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
#alpha_lamb <- alpha_lamb + T/2
beta_lamb <- 1/2*sum((logV[-1] - phi - psi*logV[-(T+1)])^2) + beta_lamb
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
print(paste(phi, psi, lamb, i))
logV_hist[i,] <- logV
phi_hist[i] <- phi
psi_hist[i] <- psi
lamb_hist[i] <- lamb
}
library(invgamma)
seed=4002; num_it=1e5
#MCMC_SV <- function(y, num_it=1e5, seed=4002){
MH_V <- function(logV_, y_t, phi, psi, lamb){
logV_old <- logV_[1]; logV_t <- logV_[2]; logV_new <- logV_[3]
mu_t <- (phi*(1-psi) + psi*(logV_new + logV_old)) / (1 + psi^2)
tilde_mu_t <- mu_t + lamb/2 * (y_t^2 * exp(-mu_t) - 1)
tilde_lambda <- lamb / (1 + psi^2)
logV_prop <- rnorm(1, tilde_mu_t, sqrt(tilde_lambda))
nominator <- exp(-y_t^2/2 * exp(-logV_prop))
denominator <- exp(-y_t^2/2 * exp(-mu_t) * (1 + mu_t - logV_prop))
target_diff <- nominator / denominator
if (!is.na(target_diff)){
if (runif(1) < min(1, target_diff)){
return (logV_prop)
}
}
return (logV_t)
return (MH_V(logV_, y_t, phi, psi, lamb))
}
# prior: phi ~ N(0, 1); psi ~ N(0, 1); sigma ~ IG(2.5, 0.025)
mu_phi <- 0; sig2_phi <- 1; mu_psi <- 0; sig2_psi <- 1; alpha_lamb <- 2.5; beta_lamb <- 0.025
T <- length(y)
set.seed(seed)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
#phi <- 0.6 ; psi <- -0.4; lamb <- 0.2
logV_hist <- matrix(0, nrow=num_it, ncol=T+1)
phi_hist <- array(phi, dim=num_it)
psi_hist <- array(psi, dim=num_it)
lamb_hist <- array(lamb, dim=num_it)
for (t in 2:T+1){
logV_hist[1,t] <- phi + psi * logV_hist[1,t-1] + sqrt(lamb) * rnorm(1, 0, 1)
}
for (i in 2:num_it){
# MH sampling for ln V
logV <- c(logV_hist[i-1,], 0)
for (t in 1:T){
# logV starts from 0; y starts from 1
# a little bug for t = 0 and t = T + 1, but run it first
logV[t+1] <- MH_V(logV[c(t, t+1, t+2)], y[t], phi, psi, lamb)
}
logV <- logV[-(T+2)]
# Gibbs sampling for parameters
mu_phi <- (sig2_phi*sum(logV[-1]-psi*logV[-(T+1)]) + lamb*mu_phi)/(T*sig2_phi + lamb)
sig2_phi <- lamb*sig2_phi/(T*sig2_phi + lamb)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
mu_psi <- (sig2_psi*sum(logV[-(T+1)]*(logV[-1]-phi)) + lamb*mu_psi) / (sig2_psi*sum(logV[-(T+1)]^2) + lamb)
sig2_psi <- lamb*sig2_psi/(sig2_psi*sum(logV[-(T+1)]^2) + lamb)
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
#alpha_lamb <- alpha_lamb + T/2
beta_lamb <- 1/2*sum((logV[-1] - phi - psi*logV[-(T+1)])^2) + beta_lamb
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
print(paste(phi, psi, lamb, i))
logV_hist[i,] <- logV
phi_hist[i] <- phi
psi_hist[i] <- psi
lamb_hist[i] <- lamb
}
set.seed(4002)
# logV starts from 0; y starts from 1
y <- array(0, 1000)
ln_v <- array(0, 1001)
for (i in 2:1001){
ln_v[i] <- 0.5 - 0.332 * ln_v[i-1] + 0.12 * rnorm(1, 0, 1)
y[i-1] <- sqrt(exp(ln_v[i])) * rnorm(1, 0, 1)
}
library(invgamma)
seed=4002; num_it=1e5
#MCMC_SV <- function(y, num_it=1e5, seed=4002){
MH_V <- function(logV_, y_t, phi, psi, lamb){
logV_old <- logV_[1]; logV_t <- logV_[2]; logV_new <- logV_[3]
mu_t <- (phi*(1-psi) + psi*(logV_new + logV_old)) / (1 + psi^2)
tilde_mu_t <- mu_t + lamb/2 * (y_t^2 * exp(-mu_t) - 1)
tilde_lambda <- lamb / (1 + psi^2)
logV_prop <- rnorm(1, tilde_mu_t, sqrt(tilde_lambda))
nominator <- exp(-y_t^2/2 * exp(-logV_prop))
denominator <- exp(-y_t^2/2 * exp(-mu_t) * (1 + mu_t - logV_prop))
target_diff <- nominator / denominator
if (!is.na(target_diff)){
if (runif(1) < min(1, target_diff)){
return (logV_prop)
}
}
return (logV_t)
return (MH_V(logV_, y_t, phi, psi, lamb))
}
# prior: phi ~ N(0, 1); psi ~ N(0, 1); sigma ~ IG(2.5, 0.025)
mu_phi <- 0; sig2_phi <- 1; mu_psi <- 0; sig2_psi <- 1; alpha_lamb <- 2.5; beta_lamb <- 0.025
T <- length(y)
set.seed(seed)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
#phi <- 0.6 ; psi <- -0.4; lamb <- 0.2
logV_hist <- matrix(0, nrow=num_it, ncol=T+1)
phi_hist <- array(phi, dim=num_it)
psi_hist <- array(psi, dim=num_it)
lamb_hist <- array(lamb, dim=num_it)
for (t in 2:T+1){
logV_hist[1,t] <- phi + psi * logV_hist[1,t-1] + sqrt(lamb) * rnorm(1, 0, 1)
}
for (i in 2:num_it){
# MH sampling for ln V
logV <- c(logV_hist[i-1,], 0)
for (t in 1:T){
# logV starts from 0; y starts from 1
# a little bug for t = 0 and t = T + 1, but run it first
logV[t+1] <- MH_V(logV[c(t, t+1, t+2)], y[t], phi, psi, lamb)
}
logV <- logV[-(T+2)]
# Gibbs sampling for parameters
mu_phi <- (sig2_phi*sum(logV[-1]-psi*logV[-(T+1)]) + lamb*mu_phi)/(T*sig2_phi + lamb)
sig2_phi <- lamb*sig2_phi/(T*sig2_phi + lamb)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
mu_psi <- (sig2_psi*sum(logV[-(T+1)]*(logV[-1]-phi)) + lamb*mu_psi) / (sig2_psi*sum(logV[-(T+1)]^2) + lamb)
sig2_psi <- lamb*sig2_psi/(sig2_psi*sum(logV[-(T+1)]^2) + lamb)
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
alpha_lamb <- alpha_lamb + T/2
beta_lamb <- 1/2*sum((logV[-1] - phi - psi*logV[-(T+1)])^2) + beta_lamb
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
print(paste(phi, psi, lamb, i))
logV_hist[i,] <- logV
phi_hist[i] <- phi
psi_hist[i] <- psi
lamb_hist[i] <- lamb
}
set.seed(4002)
# logV starts from 0; y starts from 1
y <- array(0, 1000)
ln_v <- array(0, 1001)
for (i in 2:1001){
ln_v[i] <- 0.5 - 0.332 * ln_v[i-1] + 0.12 * rnorm(1, 0, 1)
y[i-1] <- sqrt(exp(ln_v[i])) * rnorm(1, 0, 1)
}
library(invgamma)
seed=4002; num_it=1e5
#MCMC_SV <- function(y, num_it=1e5, seed=4002){
MH_V <- function(logV_, y_t, phi, psi, lamb){
logV_old <- logV_[1]; logV_t <- logV_[2]; logV_new <- logV_[3]
mu_t <- (phi*(1-psi) + psi*(logV_new + logV_old)) / (1 + psi^2)
tilde_mu_t <- mu_t + lamb/2 * (y_t^2 * exp(-mu_t) - 1)
tilde_lambda <- lamb / (1 + psi^2)
logV_prop <- rnorm(1, tilde_mu_t, sqrt(tilde_lambda))
nominator <- exp(-y_t^2/2 * exp(-logV_prop))
denominator <- exp(-y_t^2/2 * exp(-mu_t) * (1 + mu_t - logV_prop))
target_diff <- nominator / denominator
if (!is.na(target_diff)){
if (runif(1) < min(1, target_diff)){
return (logV_prop)
}
}
#return (logV_t)
return (MH_V(logV_, y_t, phi, psi, lamb))
}
# prior: phi ~ N(0, 1); psi ~ N(0, 1); sigma ~ IG(2.5, 0.025)
mu_phi <- 0; sig2_phi <- 1; mu_psi <- 0; sig2_psi <- 1; alpha_lamb <- 2.5; beta_lamb <- 0.025
T <- length(y)
set.seed(seed)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
#phi <- 0.6 ; psi <- -0.4; lamb <- 0.2
logV_hist <- matrix(0, nrow=num_it, ncol=T+1)
phi_hist <- array(phi, dim=num_it)
psi_hist <- array(psi, dim=num_it)
lamb_hist <- array(lamb, dim=num_it)
for (t in 2:T+1){
logV_hist[1,t] <- phi + psi * logV_hist[1,t-1] + sqrt(lamb) * rnorm(1, 0, 1)
}
for (i in 2:num_it){
# MH sampling for ln V
logV <- c(logV_hist[i-1,], 0)
for (t in 1:T){
# logV starts from 0; y starts from 1
# a little bug for t = 0 and t = T + 1, but run it first
logV[t+1] <- MH_V(logV[c(t, t+1, t+2)], y[t], phi, psi, lamb)
}
logV <- logV[-(T+2)]
# Gibbs sampling for parameters
mu_phi <- (sig2_phi*sum(logV[-1]-psi*logV[-(T+1)]) + lamb*mu_phi)/(T*sig2_phi + lamb)
sig2_phi <- lamb*sig2_phi/(T*sig2_phi + lamb)
phi <- rnorm(1, mean=mu_phi, sd=sqrt(sig2_phi))
mu_psi <- (sig2_psi*sum(logV[-(T+1)]*(logV[-1]-phi)) + lamb*mu_psi) / (sig2_psi*sum(logV[-(T+1)]^2) + lamb)
sig2_psi <- lamb*sig2_psi/(sig2_psi*sum(logV[-(T+1)]^2) + lamb)
psi <- rnorm(1, mean=mu_psi, sd=sqrt(sig2_psi))
alpha_lamb <- alpha_lamb + T/2
beta_lamb <- 1/2*sum((logV[-1] - phi - psi*logV[-(T+1)])^2) + beta_lamb
lamb <- rinvgamma(1, shape=alpha_lamb, rate=beta_lamb)
print(paste(phi, psi, lamb, i))
logV_hist[i,] <- logV
phi_hist[i] <- phi
psi_hist[i] <- psi
lamb_hist[i] <- lamb
}
#}
logistic <- function(x) 1/(1+exp(-x))
X <- matrix(c(0.4,0.7,0.8,0.9,1.3,1.8,-1.3,-0.9),ncol=2,byrow=T)
y <- c(0.1, -0.1, 0.95, 0.2) 			# target value
# hidden layer bias and weights
W1 <- matrix(c(0.1,-0.2,0.1,0.4,0.2,0.9),nrow=2,byrow=T)
# output layer bias and weights
W2 <- matrix(c(0.2,-0.5,0.1),nrow=1)
X1 <- cbind(1, X)
h <- logistic(W1 %*% t(X1))       # logistic hidden h'
h <- rbind(1, h)
o <- W2 %*% h                     # linear output o'
(err <- y - o)                    # output error
(mean_sse <- mean(err^2))         # mean SSE
################################################################################
lr <- 0.5                         # learning rate: $\eta$
n <- length(y)
del2 <- -2*err                    # output layer $\delta_2$
Delta_W2 <- -lr*del2 %*% t(h)     # $\Delta W2 = -\eta \delta_2 (h')^T$
new_W2 <- W2 + Delta_W2 / n       # new output weights: $W2 = W2 + \Delta W2$
del1 <- (t(W2) %*% del2)*h*(1-h)  # hidden layer $\delta_1$
del1 <- del1[-1,]                 # remove from X1
Delta_W1 <- -lr*del1 %*% X1       # $\Delta W1 = -\eta \delta_1 x^T$
new_W1 <- W1 + Delta_W1 / n       # new hidden weights: $W1 = W1 + \Delta W1$
new_h <- logistic(new_W1 %*% t(X1))
new_h <- rbind(1, new_h)
new_o <- new_W2 %*% new_h
new_err <- y - new_o
(new_mean_sse <- mean(new_err^2)) # new mean SSE
X <- matrix(c(0.4,0.7,0.8,0.9,1.3,1.8,-1.3,-0.9),ncol=2,byrow=T)
y <- c(0.1, -0.1, 0.95, 0.2) 			# target value
X <- matrix(c(0.4,0.7,0.8,0.9,1.3,1.8,-1.3,-0.9),ncol=2,byrow=T)
y <- c(0.1, -0.1, 0.95, 0.2)      # target value
library(stochvol)
?svsim
sim <- svsim(1000, mu=-10, phi=0.99, sigma=0.2)
sim
?rt
mu=-10; phi=0.99; sigma=0.2; nu=Inf
rnorm(1, mean=0, sd=sigma/sqrt(1 - phi^2))
rnorm(1, mean=mu, sd=sigma/sqrt(1 - phi^2))
set.seed(4002)
rnorm(1, mean=mu, sd=sigma/sqrt(1 - phi^2))
?svsample
svsample(sim$y, draws=10000, burnin=4000,
priormu=c(-10, 1), priorphi=c(20, 1.2), priorsigma=0.2)
svsample(sim$y, draws=30000, burnin=4000,
priormu=c(-10, 1), priorphi=c(20, 1.2), priorsigma=0.2)
logistic <- function(x) 1/(1+exp(-x))
X <- matrix(c(0.4,0.7,0.8,0.9,1.3,1.8,-1.3,-0.9),ncol=2,byrow=T)
y <- c(0, 0, 1, 0)                # target value
# hidden layer bias and weights
W1 <- matrix(c(0.1,-0.2,0.1,0.4,0.2,0.9),nrow=2,byrow=T)
# output layer bias and weights
W2 <- matrix(c(0.2,-0.5,0.1),nrow=1)
X1 <- cbind(1, X)
h <- logistic(W1 %*% t(X1))       # logistic hidden h'
h <- rbind(1, h)
o <- W2 %*% h                     # linear output o'
(err <- y - o)                    # output error
(mean_sse <- mean(err^2))         # mean SSE
################################################################################
lr <- 0.5                         # learning rate: $\eta$
n <- length(y)
del2 <- -2*err                    # output layer $\delta_2$
Delta_W2 <- -lr*del2 %*% t(h)     # $\Delta W2 = -\eta \delta_2 (h')^T$
new_W2 <- W2 + Delta_W2 / n       # new output weights: $W2 = W2 + \Delta W2$
del1 <- (t(W2) %*% del2)*h*(1-h)  # hidden layer $\delta_1$
del1 <- del1[-1,]                 # remove from X1
Delta_W1 <- -lr*del1 %*% X1       # $\Delta W1 = -\eta \delta_1 x^T$
new_W1 <- W1 + Delta_W1 / n       # new hidden weights: $W1 = W1 + \Delta W1$
new_h <- logistic(new_W1 %*% t(X1))
new_h <- rbind(1, new_h)
new_o <- new_W2 %*% new_h
new_err <- y - new_o
(new_mean_sse <- mean(new_err^2)) # new mean SSE
svsample(sim$y, draws = 10000, burnin = 5000,
priorspec = specify_priors(mu = sv_normal(-10, 1),
phi = sv_normal(0, 2),
sigma2 = sv_gamma(0.5, 2)),
parallel = "snow", n_chains = 2, n_cpus = 2)
set.seed(4002)
mu <- -0.5; phi <- 0.332; nu <- 0.12
T <- 1000
y <- array(0, T)
h <- array(0, T)
h[1] <- rnorm(1, mean=mu, sd=sqrt(nu/(1 - phi^2)))
y[1] <- exp(h[1]/2) * rnorm(1, 0, 1)
for (i in 2:T) {
h[i] <- mu + phi*(h[i-1] - mu) + sqrt(nu)*rnorm(1, 0, 1)
y[i] <- exp(h[i]/2) * rnorm(1, 0, 1)
}
svsample(y, draws=30000, burnin=4000,
priormu=c(-10, 1), priorphi=c(20, 1.2), priorsigma=0.2)
?svsim
sim <- svsim(100, mu = -10, phi = 0.99, sigma = 0.2)
## Obtain 5000 draws from the sampler (that's not a lot)
draws <-
svsample(sim, draws = 5000, burnin = 100,
priormu = c(-10, 1), priorphi = c(20, 1.5), priorsigma = 0.2)
## Check out the results
summary(draws)
sim <- svsim(1000, mu=-10, phi=0.3, sigma=0.2, nu=Inf)
svsample(sim$y, draws=30000, burnin=10000,
priormu=c(0, 100), priorphi=c(5, 1.5), priorsigma=1)
sim <- svsim(10000, mu=-3, phi=0.3, sigma=0.2, nu=Inf)
svsample(sim$y, draws=30000, burnin=10000,
priormu=c(0, 100), priorphi=c(5, 1.5), priorsigma=1)
set.seed(4002)
# y <- exp(-h/2)*rt(1, df=nu)
# h <- mu + phi*(h - mu) + sigma*rt(1, df=nu)
sim <- svsim(10000, mu=-3, phi=0.3, sigma=0.2, nu=Inf)
svsample(sim$y, draws=50000, burnin=10000,
priormu=c(0, 100), priorphi=c(5, 1.5), priorsigma=1)
